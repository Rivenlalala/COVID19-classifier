{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One-Script-to-Rule-Them-All.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rivenlalala/COVID19-classifier/blob/test/One_Script_to_Rule_Them_All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTL6T4MlXGp9",
        "colab_type": "code",
        "outputId": "fb8222e8-0b8e-49a0-ddc8-8cdaaa12fa48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/Rivenlalala/COVID19-classifier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'COVID19-classifier'...\n",
            "remote: Enumerating objects: 214, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/214)\u001b[K\rremote: Counting objects:   1% (3/214)\u001b[K\rremote: Counting objects:   2% (5/214)\u001b[K\rremote: Counting objects:   3% (7/214)\u001b[K\rremote: Counting objects:   4% (9/214)\u001b[K\rremote: Counting objects:   5% (11/214)\u001b[K\rremote: Counting objects:   6% (13/214)\u001b[K\rremote: Counting objects:   7% (15/214)\u001b[K\rremote: Counting objects:   8% (18/214)\u001b[K\rremote: Counting objects:   9% (20/214)\u001b[K\rremote: Counting objects:  10% (22/214)\u001b[K\rremote: Counting objects:  11% (24/214)\u001b[K\rremote: Counting objects:  12% (26/214)\u001b[K\rremote: Counting objects:  13% (28/214)\u001b[K\rremote: Counting objects:  14% (30/214)\u001b[K\rremote: Counting objects:  15% (33/214)\u001b[K\rremote: Counting objects:  16% (35/214)\u001b[K\rremote: Counting objects:  17% (37/214)\u001b[K\rremote: Counting objects:  18% (39/214)\u001b[K\rremote: Counting objects:  19% (41/214)\u001b[K\rremote: Counting objects:  20% (43/214)\u001b[K\rremote: Counting objects:  21% (45/214)\u001b[K\rremote: Counting objects:  22% (48/214)\u001b[K\rremote: Counting objects:  23% (50/214)\u001b[K\rremote: Counting objects:  24% (52/214)\u001b[K\rremote: Counting objects:  25% (54/214)\u001b[K\rremote: Counting objects:  26% (56/214)\u001b[K\rremote: Counting objects:  27% (58/214)\u001b[K\rremote: Counting objects:  28% (60/214)\u001b[K\rremote: Counting objects:  29% (63/214)\u001b[K\rremote: Counting objects:  30% (65/214)\u001b[K\rremote: Counting objects:  31% (67/214)\u001b[K\rremote: Counting objects:  32% (69/214)\u001b[K\rremote: Counting objects:  33% (71/214)\u001b[K\rremote: Counting objects:  34% (73/214)\u001b[K\rremote: Counting objects:  35% (75/214)\u001b[K\rremote: Counting objects:  36% (78/214)\u001b[K\rremote: Counting objects:  37% (80/214)\u001b[K\rremote: Counting objects:  38% (82/214)\u001b[K\rremote: Counting objects:  39% (84/214)\u001b[K\rremote: Counting objects:  40% (86/214)\u001b[K\rremote: Counting objects:  41% (88/214)\u001b[K\rremote: Counting objects:  42% (90/214)\u001b[K\rremote: Counting objects:  43% (93/214)\u001b[K\rremote: Counting objects:  44% (95/214)\u001b[K\rremote: Counting objects:  45% (97/214)\u001b[K\rremote: Counting objects:  46% (99/214)\u001b[K\rremote: Counting objects:  47% (101/214)\u001b[K\rremote: Counting objects:  48% (103/214)\u001b[K\rremote: Counting objects:  49% (105/214)\u001b[K\rremote: Counting objects:  50% (107/214)\u001b[K\rremote: Counting objects:  51% (110/214)\u001b[K\rremote: Counting objects:  52% (112/214)\u001b[K\rremote: Counting objects:  53% (114/214)\u001b[K\rremote: Counting objects:  54% (116/214)\u001b[K\rremote: Counting objects:  55% (118/214)\u001b[K\rremote: Counting objects:  56% (120/214)\u001b[K\rremote: Counting objects:  57% (122/214)\u001b[K\rremote: Counting objects:  58% (125/214)\u001b[K\rremote: Counting objects:  59% (127/214)\u001b[K\rremote: Counting objects:  60% (129/214)\u001b[K\rremote: Counting objects:  61% (131/214)\u001b[K\rremote: Counting objects:  62% (133/214)\u001b[K\rremote: Counting objects:  63% (135/214)\u001b[K\rremote: Counting objects:  64% (137/214)\u001b[K\rremote: Counting objects:  65% (140/214)\u001b[K\rremote: Counting objects:  66% (142/214)\u001b[K\rremote: Counting objects:  67% (144/214)\u001b[K\rremote: Counting objects:  68% (146/214)\u001b[K\rremote: Counting objects:  69% (148/214)\u001b[K\rremote: Counting objects:  70% (150/214)\u001b[K\rremote: Counting objects:  71% (152/214)\u001b[K\rremote: Counting objects:  72% (155/214)\u001b[K\rremote: Counting objects:  73% (157/214)\u001b[K\rremote: Counting objects:  74% (159/214)\u001b[K\rremote: Counting objects:  75% (161/214)\u001b[K\rremote: Counting objects:  76% (163/214)\u001b[K\rremote: Counting objects:  77% (165/214)\u001b[K\rremote: Counting objects:  78% (167/214)\u001b[K\rremote: Counting objects:  79% (170/214)\u001b[K\rremote: Counting objects:  80% (172/214)\u001b[K\rremote: Counting objects:  81% (174/214)\u001b[K\rremote: Counting objects:  82% (176/214)\u001b[K\rremote: Counting objects:  83% (178/214)\u001b[K\rremote: Counting objects:  84% (180/214)\u001b[K\rremote: Counting objects:  85% (182/214)\u001b[K\rremote: Counting objects:  86% (185/214)\u001b[K\rremote: Counting objects:  87% (187/214)\u001b[K\rremote: Counting objects:  88% (189/214)\u001b[K\rremote: Counting objects:  89% (191/214)\u001b[K\rremote: Counting objects:  90% (193/214)\u001b[K\rremote: Counting objects:  91% (195/214)\u001b[K\rremote: Counting objects:  92% (197/214)\u001b[K\rremote: Counting objects:  93% (200/214)\u001b[K\rremote: Counting objects:  94% (202/214)\u001b[K\rremote: Counting objects:  95% (204/214)\u001b[K\rremote: Counting objects:  96% (206/214)\u001b[K\rremote: Counting objects:  97% (208/214)\u001b[K\rremote: Counting objects:  98% (210/214)\u001b[K\rremote: Counting objects:  99% (212/214)\u001b[K\rremote: Counting objects: 100% (214/214)\u001b[K\rremote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (212/212), done.\u001b[K\n",
            "remote: Total 2601 (delta 3), reused 210 (delta 2), pack-reused 2387\u001b[K\n",
            "Receiving objects: 100% (2601/2601), 380.03 MiB | 37.66 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4_MzeyVGDYn",
        "colab_type": "code",
        "outputId": "b1ddc096-c3d2-4587-ed42-fde1987d51d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git checkout test"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqlkGUSTRRUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f18ad2f-7fc9-4211-d05f-8a2efb89390b"
      },
      "source": [
        "%cd COVID19-classifier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/COVID19-classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhJGqdVoeGlO",
        "colab_type": "code",
        "outputId": "8521b7b9-528d-4343-cfb5-398190cf6846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torchvision.utils import *\n",
        "from torchvision import models\n",
        "from models import *\n",
        "from utils import *\n",
        "import cv2\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "for i in range(2):\n",
        "    data = CustomCompose([transforms.RandomHorizontalFlip(),\n",
        "                        transforms.RandomRotation(10),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                                    std=[0.5, 0.5, 0.5])])\n",
        "    test = CustomCompose([transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                                std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "\n",
        "    dataset_unnormalized = CustomFolder(root='dataset/train_s', transform=data)\n",
        "    dataset_normalized = CustomFolder(root='dataset/train_sn', transform=data)\n",
        "    testset_unnormalized = CustomFolder(root='dataset/test_s', transform=test)\n",
        "    testset_normalized = CustomFolder(root='dataset/test_sn', transform=test)\n",
        "    validate_unnormalized = CustomFolder(root='dataset/train_s', transform=test)\n",
        "    validate_normalized = CustomFolder(root='dataset/train_sn', transform=test)\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"DN-u-c.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"vgg-u-c.pth\")\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_normalized, validate_normalized, testset_normalized, \"DN-n-c.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_normalized, validate_normalized, testset_normalized, \"vgg-n-c.pth\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it, loss=0.512]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 108  TN: 36  FP: 314  FN: 6\n",
            "Testing Acc: \n",
            "TP: 24  TN: 6  FP: 74  FN: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:16<00:00, 16.21s/it, loss=0.708]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 62  TN: 307  FP: 43  FN: 52\n",
            "Testing Acc: \n",
            "TP: 12  TN: 69  FP: 11  FN: 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:09<00:00,  9.28s/it, loss=0.577]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 18  TN: 342  FP: 8  FN: 96\n",
            "Testing Acc: \n",
            "TP: 3  TN: 77  FP: 3  FN: 27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:16<00:00, 16.28s/it, loss=0.748]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 109  TN: 18  FP: 332  FN: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Acc: \n",
            "TP: 30  TN: 0  FP: 80  FN: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:09<00:00,  9.25s/it, loss=0.556]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 41  TN: 334  FP: 16  FN: 73\n",
            "Testing Acc: \n",
            "TP: 9  TN: 72  FP: 8  FN: 21\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:16<00:00, 16.33s/it, loss=0.774]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 0  TN: 350  FP: 0  FN: 114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Acc: \n",
            "TP: 0  TN: 80  FP: 0  FN: 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:09<00:00,  9.28s/it, loss=0.574]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 109  TN: 37  FP: 313  FN: 5\n",
            "Testing Acc: \n",
            "TP: 28  TN: 15  FP: 65  FN: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:16<00:00, 16.25s/it, loss=0.751]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 94  TN: 70  FP: 280  FN: 20\n",
            "Testing Acc: \n",
            "TP: 29  TN: 20  FP: 60  FN: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2qKnRiNvkku",
        "colab_type": "code",
        "outputId": "06c02404-2291-4a3c-dfd7-2b62c8cb85f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "for i in range(2):\n",
        "    data = CustomCompose([transforms.RandomHorizontalFlip(),\n",
        "                        transforms.RandomRotation(10),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                                    std=[0.5, 0.5, 0.5])])\n",
        "    test = CustomCompose([transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                            std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "\n",
        "    dataset_unnormalized = CustomFolder(root='dataset/train', transform=data)\n",
        "    dataset_normalized = CustomFolder(root='dataset/train_n', transform=data)\n",
        "    testset_unnormalized = CustomFolder(root='dataset/test', transform=test)\n",
        "    testset_normalized = CustomFolder(root='dataset/test_n', transform=test)\n",
        "    validate_unnormalized = CustomFolder(root='dataset/train', transform=test)\n",
        "    validate_normalized = CustomFolder(root='dataset/train_n', transform=test)\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"DN-u-n.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"vgg-u-n.pth\")\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_normalized, validate_normalized, testset_normalized, \"DN-n-n.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_normalized, validate_normalized, testset_normalized, \"vgg-n-n.pth\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:10<00:00, 10.29s/it, loss=0.544]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 55  TN: 348  FP: 40  FN: 59\n",
            "Testing Acc: \n",
            "TP: 11  TN: 78  FP: 13  FN: 19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:17<00:00, 17.69s/it, loss=0.706]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 82  TN: 265  FP: 123  FN: 32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Acc: \n",
            "TP: 15  TN: 52  FP: 39  FN: 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:10<00:00, 10.10s/it, loss=0.57]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 29  TN: 364  FP: 24  FN: 85\n",
            "Testing Acc: \n",
            "TP: 5  TN: 87  FP: 4  FN: 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:17<00:00, 17.65s/it, loss=0.776]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 88  TN: 141  FP: 247  FN: 26\n",
            "Testing Acc: \n",
            "TP: 30  TN: 0  FP: 91  FN: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okiBNmxmfHEX",
        "colab_type": "code",
        "outputId": "fe855016-d5d5-419f-ded7-ac152d4d2bd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torchvision.utils import *\n",
        "from torchvision import models\n",
        "from models import *\n",
        "from utils import *\n",
        "import cv2\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "for i in range(2):\n",
        "    all_data = CustomFolder(root='dataset/train_s', transform=CustomCompose([]))\n",
        "    all_data_norm = CustomFolder(root='dataset/train_sn', transform=CustomCompose([]))\n",
        "\n",
        "    minority = np.array([np.array(image[0], dtype=\"float\") for image in all_data if image[1]==0])\n",
        "    minority_norm = np.array([np.array(image[0], dtype=\"float\") for image in all_data_norm if image[1]==0])\n",
        "\n",
        "    augment = CustomFolder(root=\"dataset/train_s\", transform=CustomCompose([Smote(minority, 5)]))\n",
        "    augment_norm = CustomFolder(root=\"dataset/train_sn\", transform=CustomCompose([Smote(minority, 5)]))\n",
        "\n",
        "    augmented1 = [img for img in augment if img[1] == 0]\n",
        "    augmented_norm1 = [img for img in augment_norm if img[1] == 0]\n",
        "    augmented2 = [img for img in augment if img[1] == 0]\n",
        "    augmented_norm2 = [img for img in augment_norm if img[1] == 0]\n",
        "\n",
        "    data = CustomCompose([transforms.RandomHorizontalFlip(),\n",
        "                        transforms.RandomRotation(10),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                            std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    dataset_unnormalized = CustomDataset((all_data, augmented1, augmented2), transform=data)\n",
        "    dataset_normalized = CustomDataset((all_data_norm, augmented_norm1, augmented_norm2), transform=data)\n",
        "\n",
        "    test = CustomCompose([transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                            std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    testset_unnormalized = CustomFolder(root='dataset/test_s', transform=test)\n",
        "    testset_normalized = CustomFolder(root='dataset/test_sn', transform=test)\n",
        "    validate_unnormalized = CustomFolder(root='dataset/train_s', transform=test)\n",
        "    validate_normalized = CustomFolder(root='dataset/train_sn', transform=test)\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100, dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"DN-u-smote.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"vgg-u-smote.pth\")\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100, dataset_normalized, validate_normalized, testset_normalized, \"DN-n-smote.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_normalized, validate_normalized, testset_normalized, \"vgg-n-smote.pth\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:13<00:00, 13.78s/it, loss=0.531]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 270  TN: 294  FP: 56  FN: 72\n",
            "Testing Acc: \n",
            "TP: 20  TN: 68  FP: 12  FN: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:24<00:00, 24.19s/it, loss=0.57]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 272  TN: 320  FP: 30  FN: 70\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Acc: \n",
            "TP: 11  TN: 69  FP: 11  FN: 19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:13<00:00, 13.83s/it, loss=0.485]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 220  TN: 346  FP: 4  FN: 122\n",
            "Testing Acc: \n",
            "TP: 5  TN: 79  FP: 1  FN: 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:24<00:00, 24.43s/it, loss=0.505]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 202  TN: 345  FP: 5  FN: 140\n",
            "Testing Acc: \n",
            "TP: 3  TN: 80  FP: 0  FN: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHwfc0Gdz1Hp",
        "colab_type": "code",
        "outputId": "d92f61ef-0d77-4e36-da7b-b3412784aa8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torchvision.utils import *\n",
        "from torchvision import models\n",
        "from models import *\n",
        "from utils import *\n",
        "import cv2\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "for i in range(2)\n",
        "    all_data = CustomFolder(root='dataset/train_s', transform=CustomCompose([]))\n",
        "    all_data_norm = CustomFolder(root='dataset/train_sn', transform=CustomCompose([]))\n",
        "\n",
        "    raw = np.array([np.array(image[0], dtype=\"float\") for image in all_data])\n",
        "    raw_norm = np.array([np.array(image[0], dtype=\"float\") for image in all_data_norm])\n",
        "\n",
        "    augment = CustomFolder(root=\"dataset/train_s\", transform=CustomCompose([SampleParing(raw, minority_only=False)]))\n",
        "    augment_norm = CustomFolder(root=\"dataset/train_sn\", transform=CustomCompose([SampleParing(raw_norm, minority_only=False)]))\n",
        "\n",
        "    augmented1 = [img for img in augment]\n",
        "    augmented_norm1 = [img for img in augment_norm]\n",
        "    augmented2 = [img for img in augment]\n",
        "    augmented_norm2 = [img for img in augment_norm]\n",
        "\n",
        "    data = CustomCompose([transforms.RandomHorizontalFlip(),\n",
        "                        transforms.RandomRotation(10),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                            std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    dataset_unnormalized = CustomDataset((all_data, augmented1, augmented2), transform=data)\n",
        "    dataset_normalized = CustomDataset((all_data_norm, augmented_norm1, augmented_norm2), transform=data)\n",
        "\n",
        "    test = CustomCompose([transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                            std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    testset_unnormalized = CustomFolder(root='dataset/test_s', transform=test)\n",
        "    testset_normalized = CustomFolder(root='dataset/test_sn', transform=test)\n",
        "    validate_unnormalized = CustomFolder(root='dataset/train_s', transform=test)\n",
        "    validate_normalized = CustomFolder(root='dataset/train_sn', transform=test)\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"DN-u-sp.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"vgg-u-sp.pth\")\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_normalized, validate_normalized, testset_normalized, \"DN-n-sp.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_normalized, validate_normalized, testset_normalized, \"vgg-n-sp.pth\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:27<00:00, 27.49s/it, loss=0.56]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 4  TN: 1048  FP: 2  FN: 338\n",
            "Testing Acc: \n",
            "TP: 0  TN: 80  FP: 0  FN: 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:48<00:00, 48.61s/it, loss=0.709]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 237  TN: 672  FP: 378  FN: 105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Acc: \n",
            "TP: 16  TN: 59  FP: 21  FN: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:27<00:00, 27.16s/it, loss=0.549]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 48  TN: 1010  FP: 40  FN: 294\n",
            "Testing Acc: \n",
            "TP: 2  TN: 80  FP: 0  FN: 28\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:49<00:00, 49.08s/it, loss=0.733]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Training Acc: \n",
            "TP: 201  TN: 723  FP: 327  FN: 141\n",
            "Testing Acc: \n",
            "TP: 13  TN: 64  FP: 16  FN: 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORan1Qa62LHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "40dbca89-5ca2-45b8-e655-8040ee89e6dc"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torchvision.utils import *\n",
        "from torchvision import models\n",
        "from models import *\n",
        "from utils import *\n",
        "import cv2\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "for i in range(2):\n",
        "    all_data = CustomFolder(root='dataset/train_s', transform=CustomCompose([]))\n",
        "    all_data_norm = CustomFolder(root='dataset/train_sn', transform=CustomCompose([]))\n",
        "\n",
        "    augment = CustomFolder(root=\"dataset/train_s\", transform=CustomCompose([RICAP(all_data)]))\n",
        "    augment_norm = CustomFolder(root=\"dataset/train_sn\", transform=CustomCompose([RICAP(all_data_norm)]))\n",
        "\n",
        "    augmented1 = [img for img in augment]\n",
        "    augmented_norm1 = [img for img in augment_norm]\n",
        "    augmented2 = [img for img in augment]\n",
        "    augmented_norm2 = [img for img in augment_norm]\n",
        "\n",
        "    data = CustomCompose([transforms.RandomHorizontalFlip(),\n",
        "                        transforms.RandomRotation(10),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                            std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    dataset_unnormalized = CustomDataset((all_data, augmented1, augmented2), transform=data)\n",
        "    dataset_normalized = CustomDataset((all_data_norm, augmented_norm1, augmented_norm2), transform=data)\n",
        "\n",
        "    test = CustomCompose([transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                            std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    testset_unnormalized = CustomFolder(root='dataset/test_s', transform=test)\n",
        "    testset_normalized = CustomFolder(root='dataset/test_sn', transform=test)\n",
        "    validate_unnormalized = CustomFolder(root='dataset/train_s', transform=test)\n",
        "    validate_normalized = CustomFolder(root='dataset/train_sn', transform=test)\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"DN-u-R.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"vgg-u-R.pth\")\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_normalized, validate_normalized, testset_normalized, \"DN-n-R.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_normalized, validate_normalized, testset_normalized, \"vgg-n-R.pth\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:27<00:00, 27.34s/it, loss=0.518]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "DN-u-R.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 40  TN: 581  FP: 98  FN: 673\n",
            "Testing Acc: \n",
            "TP: 14  TN: 71  FP: 9  FN: 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:47<00:00, 47.92s/it, loss=0.695]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "vgg-u-R.pth\n",
            "Training Acc: \n",
            "TP: 90  TN: 448  FP: 497  FN: 357\n",
            "Testing Acc: \n",
            "TP: 15  TN: 56  FP: 24  FN: 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1:   0%|          | 0/1 [00:17<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9cfff137bae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mDN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdataset_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DN-n-R.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mVGG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVGG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vgg-n-R.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/COVID19-classifier/utils.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, epoch, dataset, validate, testset, filename)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ikmuVwm-r7Q",
        "colab_type": "code",
        "outputId": "29676ad6-b48f-4394-c237-1334539e48c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "from utils import *\n",
        "from models import *\n",
        "import cv2\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "for i in range(2):\n",
        "    all_data = CustomFolder(root='dataset/train_s', transform=CustomCompose([]))\n",
        "    all_data_norm = CustomFolder(root='dataset/train_sn', transform=CustomCompose([]))\n",
        "\n",
        "    minority = np.array([np.array(image[0], dtype=\"float\") for image in all_data if image[1]==0])\n",
        "    minority_norm = np.array([np.array(image[0], dtype=\"float\") for image in all_data_norm if image[1]==0])\n",
        "\n",
        "    augment = CustomFolder(root=\"dataset/train_s\", transform=CustomCompose([Majority(minority)]))\n",
        "    augment_norm = CustomFolder(root=\"dataset/train_sn\", transform=CustomCompose([Majority(minority_norm)]))\n",
        "\n",
        "    augmented1 = [img for img in augment if img[1] == 0]\n",
        "    augmented_norm1 = [img for img in augment_norm if img[1] == 0]\n",
        "\n",
        "    data = CustomCompose([transforms.RandomHorizontalFlip(),\n",
        "                        transforms.RandomRotation(10),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                            std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    dataset_unnormalized = CustomDataset((all_data, augmented1, augmented2), transform=data)\n",
        "    dataset_normalized = CustomDataset((all_data_norm, augmented_norm2, augmented_norm2), transform=data)\n",
        "\n",
        "    test = CustomCompose([transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                            std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    testset_unnormalized = CustomFolder(root='dataset/test_s', transform=test)\n",
        "    testset_normalized = CustomFolder(root='dataset/test_sn', transform=test)\n",
        "    validate_unnormalized = CustomFolder(root='dataset/train_s', transform=test)\n",
        "    validate_normalized = CustomFolder(root='dataset/train_sn', transform=test)\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"DN-u-M.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_unnormalized, validate_unnormalized, testset_unnormalized, \"vgg-u-M.pth\")\n",
        "\n",
        "    DN = DenseNet121().cuda()\n",
        "    training(DN, 100,  dataset_normalized, validate_normalized, testset_normalized, \"DN-n-M.pth\")\n",
        "    VGG = VGG16().cuda()\n",
        "    training(VGG, 100, dataset_normalized, validate_normalized, testset_normalized, \"vgg-n-M.pth\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:22<00:00, 22.21s/it, loss=0.523]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "DN-u-M.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc: \n",
            "TP: 0  TN: 350  FP: 0  FN: 772\n",
            "Testing Acc: \n",
            "TP: 0  TN: 80  FP: 0  FN: 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1: 100%|██████████| 1/1 [00:39<00:00, 39.11s/it, loss=0.645]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "vgg-u-M.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fcf24d9e0590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdataset_unnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_unnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset_unnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DN-u-M.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mVGG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVGG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_unnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_unnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset_unnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vgg-u-M.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mDN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/COVID19-classifier/utils.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, epoch, dataset, validate, testset, filename)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;31m#model = best_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mprint_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/COVID19-classifier/utils.py\u001b[0m in \u001b[0;36mprint_acc\u001b[0;34m(model, dataloader, testloader)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVpWTVjULjXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}